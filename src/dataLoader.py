
import config as config
import pandas as pd
import os
import pyspark as ps
from  pyspark.sql  import SparkSession
from pyspark.conf import SparkConf
from pyspark import SparkContext
from pyspark.sql.functions import Column as col,udf
from utils import  DataPreprocessingModule,FeatureEngineeringModule
from utils import save_pickle,load_pickle
import easydict as edict
import logging
import time

class DataLoader(object):
    def __init__(self,datapath):
        self.datapath = datapath  
        # recording the continuous var and categorycal var generated by preprocessing and featureEngineering
        self.newCategoryCols = []
        self.newContinueCols = []
    
    def _preprocessing(self):
        # deal with timestamp
        
        dataprocessing  = DataPreprocessingModule(self.orgData)
        

        dataprocessing._timePreprocessing("context_timestamp")
        logging.info('time_stamp preprocessing Done.')

        dataprocessing._categoryPreprocessing('item_category_list')
        logging.info('item_category_list Done.')

        dataprocessing._propertyPreprocessing('item_property_list')
        logging.info('property dealing Done.')

        dataprocessing._predictCategory('predict_category_property')
        # logging.info('predict catetory Done.')

        dataprocessing._labelEncoder(config.COL_NEED_ENCODE)
        dataprocessing._labelEncoder(dataprocessing.newCategoryCols)
        logging.info('labelEncoder Done.')


        self.preprocessingData = dataprocessing.data
        self.newCategoryCols.extend(dataprocessing.newCategoryCols)
        self.newContinueCols.extend(dataprocessing.newContinueCols)

        logging.info('preprocessing Done.')

    def _featureEngineering(self):
        featureEngineering = FeatureEngineeringModule(self.preprocessingData)
        featureEngineering._genExposeRatioThroughSth(
            [
                ['item_id','hour'],['item_id','maphour'],
                ['user_id','hour'],['user_id','maphour']
            ]
        )
        logging.info("genExposeRatioThroughTime Done.")
        logging.info("data shape is enlarge into " +str(featureEngineering.data.shape[1]))

        featureEngineering._genAggregateLevelThroughSth(
            {
                "hour":["user_id","item_id","shop_id"],
                "user_age_level":["item_id","item_brand_id","shop_id"]
            }
        )
        logging.info("genAggregateLevelThoughSth Done")
        logging.info("data shape is enlarge into " +str(featureEngineering.data.shape[1]))


        featureEngineering._genActivePerTimeGrain(
            {
                "day":{
                    "item_category_list":["item_id","user_id"],
                    "user_id":["item_city_id","shop_id","item_brand_id"],
                    "item_id":["user_id"],
                    "shop_id":["user_id"],
                    "item_brand_id":["user_id"]
                }
            }

        )
        logging.info("genActivePerTimeGrain Done.")
        logging.info("data shape is enlarge into " +str(featureEngineering.data.shape[1]))

        # featureEngineering._genMaxBofA(
        #     {
        #         "user_id":[
        #             item_city_id","shop_id","item_brand_id"
        #         ],
        #         "item_id":[
        #             "shop_id"
        #         ]
        #     }
        # )

        featureEngineering._genRelativeRatioToAvg(
            {
                "item_brand_id":["item_price_level","item_sales_level","item_collected_level"]
                ,"item_city_id":["item_price_level","item_sales_level","item_collected_level"]
                ,"item_category_list":["item_price_level","item_sales_level","item_collected_level"]
            }
        )

        logging.info("genRelativeRatioToAvg Done.")
        logging.info("data shape is enlarge into " +str(featureEngineering.data.shape[1]))
        
        featureEngineering.doTricks1()
        featureEngineering.doTricks2()
        logging.info("tricks Done!")
        logging.info("data shape is enlarge into " +str(featureEngineering.data.shape[1]))


        featureEngineering.lasttimeDiff()
        featureEngineering.nexttimeDiff()

        featureEngineering.dorollWin()
        featureEngineering.doSize()
        logging.info("time features is extracted ,Done")
        logging.info("data shape is enlarge into " +str(featureEngineering.data.shape[1]))


        featureEngineering.diffWithLastView(grainedList  = [
            "shop_id","item_brand_id","item_sales_level"
        ])

        logging.info("sucess !!")
        self.featureEngineeringData = featureEngineering.data
        self.newCategoryCols.extend(featureEngineering.newCategoryCols)
        self.newContinueCols.extend(featureEngineering.newContinueCols)


    def load_data(self,useCV = False ,useSpark = True,nrows = -1,interactive = True):
        if useSpark:
            self._init_sparkSession("ijcaiCode")
            # obtain original data
            if nrows == -1:
                self.orgData = self.sess.read.csv(self.datapath,sep = ' ',header=True)
            else:
                self.orgData = self.sess.read.csv(self.datapath,sep = ' ',header=True,nrows = nrows)

                logging.info("original data shape is " + str(self.orgData.shape[1]) + "with " + str(self.orgData.shape[0]) + " rows")

            # # self.orgData = self.orgData.toPandas()
            # data = self.orgData.select('context_timestamp').limit(1000)
            # from pyspark.sql.functions import udf
            # import pyspark.sql.functions as F
            # from pyspark.sql.types import DateType
            # data = data.withColumn("day",F.from_unixtime('context_timestamp').cast(DateType())).show()


            print("hello")

        else:
            if nrows == -1:
                self.orgData = pd.read_csv(self.datapath,sep = ' ')
            else:
                self.orgData = pd.read_csv(self.datapath,sep = ' ', nrows = nrows)

                logging.info("original data shape is " + str(self.orgData.shape[1]) + "with " + str(self.orgData.shape[0]) + " rows")
            if interactive:
                # defined the cols type interacitvely
                for cols in self.orgData.columns:
                    print(self.orgData.loc[:,cols].head())
                    print("please choose the type of cols:")
                    typeOfCol = input("CATEGORY : CHOOSE 1  ;\nCONTINUE : CHOOSE 2 ; \nSTR: CHOOSE 3 ;\nOTHER : CHOOSE 4 \n ")
                    if int(typeOfCol) == 1:
                        config.COLS_WITH_CATEGORY_TYPE.append(cols)
                    elif int(typeOfCol) == 2 :
                        config.COLS_WITH_CONTINUE_TYPE.append(cols)
                    elif int(typeOfCol) == 3:
                        config.COLS_WITH_STR_TYPE.append(cols)
                    else:
                        config.COLS_WITH_OTHER_TYPE.append(cols)
                print("category : ",config.COLS_WITH_CATEGORY_TYPE)
                print("continue : ",config.COLS_WITH_CONTINUE_TYPE)
                print("str :",config.COLS_WITH_STR_TYPE)
                print("other :",config.COLS_WITH_OTHER_TYPE)
                raise Exception("now you can fill the list in config with output")
                        
            else:
                # defined the type of cols in config.py derectly                
                pass


            # do some preprocession here
            # what we call preprocessing, is those work which is related to data cleaning
            # filling na , deal with outliers and others
            self._preprocessing()

            # do some feature generation here
            # this process is used to gnerate some basic feature
            self._featureEngineering()

            # use Data to return for the next step
            self.label = self.featureEngineeringData.pop('is_trade')

            return Data(
                        data = self.featureEngineeringData,
                        label = self.label,
                        newCategoryCols = self.newCategoryCols,
                        newContinueCols = self.newContinueCols)

    def _init_sparkSession(self,appName):

        self.sc   = SparkContext(master='local',appName=appName)
        self.sess = SparkSession.builder.appName("dataLoader").config(conf = SparkConf()).getOrCreate()


class Data(object):
    def __init__(self,
                 data = None,
                 label = None,
                 categoryCols = None,
                 continueCOls = None,
                 useLessCols = None,
                 useConfig = True,
                 newCategoryCols = None ,
                 newContinueCols = None
                 ):
        self.data  = data
        self.label = label
        if useConfig:
            self.categoryCols = config.COLS_WITH_CATEGORY_TYPE
            self.continueCols = config.COLS_WITH_CONTINUE_TYPE
            self.useLessCols  = config.COLS_THAT_USELESS
        else :
            self.categoryCols = categoryCols
            self.continueCols = continueCOls
            self.useLessCols  = useLessCols

        if newCategoryCols != None :
            self.categoryCols.extend(newCategoryCols)
        if newContinueCols != None:
            self.continueCols.extend(newContinueCols)
        

        self.train_data = None
        self.test_data = None 
        self.train_label = None
        self.test_label = None



    def getVariable(self,printVar = True):
        if printVar:
            print("category var are : ") 
            for i in self.categoryCols:
                print(i)
            print("\n\n\n")
            print("continue var are : ")
            for i in self.continueCols:
                print(i)

        return self.categoryCols,self.continueCols

    def setUselessVar(self,uselessList = None):
        if uselessList != None:
            self.useLessCols.extend(uselessList)
            # getting the data you may used
        
        otherUseless =set(self.data.columns).difference(self.data.select_dtypes(include = ['int','float','bool']).columns)
        self.useLessCols = list(set(self.useLessCols).union(otherUseless))
        self.data = self.data.loc[:,~self.data.columns.isin(self.useLessCols)]

    def useBestVariable(self):
        usefulvar = config.BEST_VARIABLE
        usefulvar.extend(self.categoryCols)
        self.data = self.data.loc[:,usefulvar]
        


    def encodingCategorycalData(self,method = "onehot"):
        pass
    def to_lightgbm_data(self):
        pass

    def to_xgb_data(self):
        pass


    def train_val_split(self,percent = 0.2): 
        pass

    def train_val_split_by_time(self,grain  = "day"):
        lastday = self.data[grain].max()
        self.train_data = self.data.loc[self.data[grain] != lastday ,:]
        self.train_label = self.label.loc[self.data[grain] != lastday]
        self.test_data = self.data.loc[self.data[grain] == lastday,:]
        self.test_label = self.label.loc[self.data[grain] == lastday]
        
        logging.info("split data by time , Done")
        logging.info("train set has row : "+str(self.train_data.shape[0]))
        logging.info("test set has row : "+str(self.test_data.shape[0]))

    def getSplitedData(self):
        return self.train_data,self.train_label,self.test_data,self.test_label
        

    def cvIter(self):
        # generate stratified cv iteration here , for grid searching
        pass
    def downSampleIter(self):
        assert self.train_data != None 
    def save_data(self,saveFolder = ''):
        assert saveFolder != '' , "you need to sepcify folder !!"

        saving_path = os.path.join(saveFolder,'data.pkl')
        save_pickle(self,saving_path)


    def load_data(self,loadFolder = ''):
        assert loadFolder != '', "choose a folder please"
        # usage : 
        # data = Data()
        # data = data.load_data(


        loading_path = os.path.join(loadFolder,'data.pkl')
        return load_pickle(loading_path)

